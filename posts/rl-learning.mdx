---
title: "RL Learning"
description: "Tracking my learning of RL and getting good (generally)"
slug: rl-learning
date: "March 10, 2025"
categories: ["RL"]
keywords: ["RL"]
topic: "Neo Gap Semester"
---

# Introduction

This outlines my work with learning RL and whether it's worth pursuing further.

## March 10, 2025

Started working on the OpenAI Spinning Up. Today I went through the section on Kinds of RL Algorithms, 
Policy Optimization, and then looked at some code that implements basic policy gradient methods. Worked through the math 
of each step to understand the derivation. 

Read DQN paper. Clever tricks implemented with the "experience replay" to update the network. ]

Met with Marcus to discuss why is this interesting and what will it lead to:
- My immediate conclusion of a "risk" would be attepmting to implement MuZero and beat the paper. This is a fine goal. However, need a few things to be true:
    - Feasible within a budget (likely $5k in modal credits is the upper bound)
    - Feasible within 1-2 months
    - Must be absolutely stoked on it. 
    - Is sufficiently risky and has some interesting outcome. 
- Think about why nothing much has come of MuZero after the paper. Talk to people! Is it worth learning more about RL if it's not going to lead anywhere? 
    - Cold outreach to everyone who wrote the paper.
    - Look at who's citing it recently. 
    - Who is making improvements? Where? 
    - DM Brad Porter on Slack. 


What do i need to do tomorrow? 
- Keep working on spinning up.
- Cold outreach to all the people above. 
- Figure out how feasible/risky is the MuZero/EfficientZero implementation (esp. w.r.t my time in SF, is it worth it? Is it sufficiently risky?)
- What problems can be solved with RL that are meaningful? 
    - [Testing Casual hypothesis through hierarchical RL](https://openreview.net/pdf?id=ZqNcJ8uuHT)

## March 11, 2025

Worked with Harry on a tech spech for something he's doing. To get there, I read these papers:
- Skimmed DeepSeek Prover.
- [REFACTOR](https://www.semanticscholar.org/reader/5e7999443d37916269db5ff758587335335ff75d) paper - Learning to extract theorems from proofs. 
- Google's [PAIRED](https://research.google/blog/paired-a-new-multi-agent-approach-for-adversarial-environment-generation/) blog post.

Talked with Pavla about thoughts moving forward. Conclusions:
- Keep working on spinning up. 
- Start reaching out to professors at UVA doing deep learning/RL. 
- Cold outreach to people who wrote the MuZero paper. 
- Keep writing. 




