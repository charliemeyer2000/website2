---
title: "RL Learning"
description: "Tracking my learning of RL and getting good (generally)"
slug: rl-learning
date: "March 10, 2025"
categories: ["RL"]
keywords: ["RL"]
topic: "Neo Gap Semester"
---

# Introduction

This outlines my work with learning RL and whether it's worth pursuing further.

## March 10, 2025

Started working on the OpenAI Spinning Up. Today I went through the section on Kinds of RL Algorithms, 
Policy Optimization, and then looked at some code that implements basic policy gradient methods. Worked through the math 
of each step to understand the derivation. 

Read DQN paper. Clever tricks implemented with the "experience replay" to update the network. ]

Met with Marcus to discuss why is this interesting and what will it lead to:
- My immediate conclusion of a "risk" would be attepmting to implement MuZero and beat the paper. This is a fine goal. However, need a few things to be true:
    - Feasible within a budget (likely $5k in modal credits is the upper bound)
    - Feasible within 1-2 months
    - Must be absolutely stoked on it. 
    - Is sufficiently risky and has some interesting outcome. 
- Think about why nothing much has come of MuZero after the paper. Talk to people! Is it worth learning more about RL if it's not going to lead anywhere? 
    - Cold outreach to everyone who wrote the paper.
    - Look at who's citing it recently. 
    - Who is making improvements? Where? 
    - DM Brad Porter on Slack. 


What do i need to do tomorrow? 
- Keep working on spinning up.
- Cold outreach to all the people above. 
- Figure out how feasible/risky is the MuZero/EfficientZero implementation (esp. w.r.t my time in SF, is it worth it? Is it sufficiently risky?)
- What problems can be solved with RL that are meaningful? 
    - [Testing Casual hypothesis through hierarchical RL](https://openreview.net/pdf?id=ZqNcJ8uuHT)

## March 11, 2025

Worked with Harry on a tech spech for something he's doing. To get there, I read these papers:
- Skimmed DeepSeek Prover.
- [REFACTOR](https://www.semanticscholar.org/reader/5e7999443d37916269db5ff758587335335ff75d) paper - Learning to extract theorems from proofs. 
- Google's [PAIRED](https://research.google/blog/paired-a-new-multi-agent-approach-for-adversarial-environment-generation/) blog post.
- STP: Self-play LLM theorem prover.

Did longest substring without repeating characters LC #3.

Talked with Pavla about thoughts moving forward. Conclusions:
- Keep working on spinning up. 
- Start reaching out to professors at UVA doing deep learning/RL. 
- Cold outreach to people who wrote the MuZero paper. 
- Keep writing. 

- [x] Standard architectures 
    - [x] Multi-layer perceptron - multi layer neural network: http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/
    - [x] [Vanilla RNN](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
        - [ ] TODO: Need to review this code, both the 100-line gist and the larger implementation. 
    - [ ] [LSTM](https://arxiv.org/abs/1503.04069)
- [ ] GRU - Gated Recurrent Unit
- [ ] CNNs / conv layers:
    - https://colah.github.io/posts/2014-07-Conv-Nets-Modular/
    - https://cs231n.github.io/convolutional-networks/
- [ ] resnets:
    - https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
    - https://arxiv.org/pdf/1512.03385
- [ ] attention mechanisms + transformers
    - Review
- [ ] regularizaiton methods
    - weight decay
    - dropout
- [ ] normalization methods
    - batch norm: https://arxiv.org/abs/1502.03167
    - layer norm: https://arxiv.org/abs/1607.06450
    - weight norm: https://arxiv.org/abs/1602.07868
- [ ] optimizers
    - Adam: https://arxiv.org/abs/1412.6980
    - SGD: http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/
- [ ] reparametrization trick (auto-encoding variational bayes)
    - Paper that implements it: https://arxiv.org/abs/1312.6114
    - Blog post: https://gregorygundersen.com/blog/2018/04/29/reparameterization/
        - Prerequisites:
            - Variational Inference: https://arxiv.org/pdf/1601.00670
            - Tutorial on variational autoencoders: https://arxiv.org/pdf/1606.05908
            - AlexNet: https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf


## March 12, 2025

Worked through backprop in the MLP article, along with following 3Blue1Brown's video. 
Did a demo of Hufflo to a ecomm community, led to lots of new customers. 


## March 13, 2025

Partner came so was out, working out of coffee shop. 

Read long blog post sent by Harry on [KL Divergence](https://blog.alexalemi.com/kl-is-all-you-need.html)
created smaller list of professors at UVA to reach out to. 
for each professor, have list of grad students to talk to.

## March 14, 2025 - Day off. 

## March 17

- Worked thorugh a review of variational inference
- Worked through VAESs and reparameterization trick, including (brief) derivations. 
- Read through [doc on CNNs](https://cs231n.github.io/convolutional-networks/)

## March 18

Spent all day cooking on [Savant](https://github.com/toni-akintola/savant-io).

## March 19

- Spent half the day cooking on Savant, the other half of the day finishing up vanilla rnn blog post by Karpathy. 


## March 20

- Morning at GTC, talked with Jensen. Got backt to the office at 3:00

### Email to Professors (general)

#### Email to Shangton Zhang (shangtong@virginia.edu)

TITLE: [e5274d52b4c6] Undergraduate Research Intern Inquiry

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm away from Grounds taking a 
semester off on a fellowship through the [Neo Scholars](https://neo.com/scholars) program to explore my interests. 

I have a strong background in software engineering and full stack development, but throughout my time away, 
I've become increasingly curious about machine learning and reinforcement learning. I'm working through OpenAI's "Spinning Up" 
along wtih Sutton and Barto's Introduction to RL textbook, with my goal of returning to Grounds in the fall of 2025 having a strong
foundation in RL and the math that motivates it. 

I want to work for you because: 
- I'm 










