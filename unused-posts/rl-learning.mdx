---
title: "RL Learning"
description: "Tracking my learning of RL and getting good (generally)"
slug: rl-learning
date: "March 10, 2025"
categories: ["RL"]
keywords: ["RL"]
topic: "Neo Gap Semester"
---

# Introduction

This outlines my work with learning RL and whether it's worth pursuing further.

## March 10, 2025

Started working on the OpenAI Spinning Up. Today I went through the section on Kinds of RL Algorithms, 
Policy Optimization, and then looked at some code that implements basic policy gradient methods. Worked through the math 
of each step to understand the derivation. 

Read DQN paper. Clever tricks implemented with the "experience replay" to update the network. ]

Met with Marcus to discuss why is this interesting and what will it lead to:
- My immediate conclusion of a "risk" would be attepmting to implement MuZero and beat the paper. This is a fine goal. However, need a few things to be true:
    - Feasible within a budget (likely $5k in modal credits is the upper bound)
    - Feasible within 1-2 months
    - Must be absolutely stoked on it. 
    - Is sufficiently risky and has some interesting outcome. 
- Think about why nothing much has come of MuZero after the paper. Talk to people! Is it worth learning more about RL if it's not going to lead anywhere? 
    - Cold outreach to everyone who wrote the paper.
    - Look at who's citing it recently. 
    - Who is making improvements? Where? 
    - DM Brad Porter on Slack. 


What do i need to do tomorrow? 
- Keep working on spinning up.
- Cold outreach to all the people above. 
- Figure out how feasible/risky is the MuZero/EfficientZero implementation (esp. w.r.t my time in SF, is it worth it? Is it sufficiently risky?)
- What problems can be solved with RL that are meaningful? 
    - [Testing Casual hypothesis through hierarchical RL](https://openreview.net/pdf?id=ZqNcJ8uuHT)

## March 11, 2025

Worked with Harry on a tech spech for something he's doing. To get there, I read these papers:
- Skimmed DeepSeek Prover.
- [REFACTOR](https://www.semanticscholar.org/reader/5e7999443d37916269db5ff758587335335ff75d) paper - Learning to extract theorems from proofs. 
- Google's [PAIRED](https://research.google/blog/paired-a-new-multi-agent-approach-for-adversarial-environment-generation/) blog post.
- STP: Self-play LLM theorem prover.

Did longest substring without repeating characters LC #3.

Talked with Pavla about thoughts moving forward. Conclusions:
- Keep working on spinning up. 
- Start reaching out to professors at UVA doing deep learning/RL. 
- Cold outreach to people who wrote the MuZero paper. 
- Keep writing. 

- [x] Standard architectures 
    - [x] Multi-layer perceptron - multi layer neural network: http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/
    - [x] [Vanilla RNN](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
        - [ ] TODO: Need to review this code, both the 100-line gist and the larger implementation. 
    - [X] [LSTM](https://arxiv.org/abs/1503.04069)
- [ ] GRU - Gated Recurrent Unit
- [ ] CNNs / conv layers:
    - https://colah.github.io/posts/2014-07-Conv-Nets-Modular/
    - https://cs231n.github.io/convolutional-networks/
- [ ] resnets:
    - https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
    - https://arxiv.org/pdf/1512.03385
- [ ] attention mechanisms + transformers
    - Review
- [ ] regularizaiton methods
    - weight decay
    - dropout
- [ ] normalization methods
    - batch norm: https://arxiv.org/abs/1502.03167
    - layer norm: https://arxiv.org/abs/1607.06450
    - weight norm: https://arxiv.org/abs/1602.07868
- [ ] optimizers
    - Adam: https://arxiv.org/abs/1412.6980
    - SGD: http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/
- [ ] reparametrization trick (auto-encoding variational bayes)
    - Paper that implements it: https://arxiv.org/abs/1312.6114
    - Blog post: https://gregorygundersen.com/blog/2018/04/29/reparameterization/
        - Prerequisites:
            - Variational Inference: https://arxiv.org/pdf/1601.00670
            - Tutorial on variational autoencoders: https://arxiv.org/pdf/1606.05908
            - AlexNet: https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf


## March 12, 2025

Worked through backprop in the MLP article, along with following 3Blue1Brown's video. 
Did a demo of Hufflo to a ecomm community, led to lots of new customers. 

## March 13, 2025

Partner came so was out, working out of coffee shop. 

Read long blog post sent by Harry on [KL Divergence](https://blog.alexalemi.com/kl-is-all-you-need.html)
created smaller list of professors at UVA to reach out to. 
for each professor, have list of grad students to talk to.

## March 14, 2025 - Day off. 

## March 17

- Worked thorugh a review of variational inference
- Worked through VAESs and reparameterization trick, including (brief) derivations. 
- Read through [doc on CNNs](https://cs231n.github.io/convolutional-networks/)

## March 18

Spent all day cooking on [Savant](https://github.com/toni-akintola/savant-io).

## March 19

- Spent half the day cooking on Savant, the other half of the day finishing up vanilla rnn blog post by Karpathy. 


## March 20

- Morning at GTC, talked with Jensen. Got backt to the office at 3:00
- Spent the afternoon working on emails to professors & who to reach out to. 

### Email to Professors (general)

#### Email to [Shangton Zhang [(shangtong@virginia.edu)](https://shangtongzhang.github.io/)

##### Draft 1 - Shangon Zhang
 
TITLE: [e5274d52b4c6] Undergraduate Research Intern Inquiry

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm away from Grounds taking a 
semester off on a fellowship through the [Neo Scholars](https://neo.com/scholars) program to explore my interests. 

I have a strong background in software engineering and full stack development, but throughout my fellowship, 
I've become increasingly curious about machine learning and reinforcement learning. I'm working through OpenAI's "Spinning Up" 
along wtih Sutton and Barto's Introduction to RL textbook, with my goal of returning to Grounds in the fall of 2025 having a strong
foundation in RL and the math that motivates it. 

Why I will be helpful for you: 
- I will be a part-time student for an entire year so I'm willing
- I have backgrounds in the industry in software engineering working in small/large teams. 
- I've served ML workloads with Ray. 
- I'm a fast learner. 

Why I care about RL:
- I care about the beauty of the cleanliness of RL, and I enjoy the math behind it. 
- There are lots of interesting problems in RL:
    - Working in noisy environments
    - Sample efficiency
    - Model-free RL (the MuZero paper was mind-blowing to me!)
- RL "feels" less hand-wavey than LM's. 

Also, I'd love to talk with you in person in addition to a Zoom call. 
I'll be returning to UVA on March 29-30th and would love to chat in person if you're interested. 

Thank you, 

- I have a lot to learn from you
- your research direction xyz
- what is cool shit your grad students are doing? 


##### Draft 2 - Shangton Zhang

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm currently on a fellowship through the 
[Neo Scholars](https://neo.com/scholars) program and have spent my time away from Grounds exploring 
machine learning and reinforcement learning, and would love to intern for you. 

Why you would benefit from me:
- I will be part-time student for an entire year and willing to dedicate 40+ hours/week to research.
- I have this remaining semester with the fellowship, along with a summer to dedicate to spinning up in RL. 
- I have worked in small and large teams in the industry as a software engineer at 
[Principal Financial Group](https://www.principal.com/) and [Vercel](https://vercel.com).
- I've worked on various ML projects:
    - Fine-tuned diffusion models (worked with Rivanna).
    - Served ML workloads with Ray.
    - Completed simple RL experiments with OpenAI gymnasium. 

Qualitative skills:
- I'm a fast learner. 
- I have a strong foundation in math. 
- I'm curious and eager to explore research (prefer to move away from traditional SWE web/fullstack work).

What I'm working on to get up to speed:
- OpenAI's Spinning Up in RL.
- Sutton and Barto's Textbook (accompanied with your GitHub repo of the python replication).
- Building towards an implementation of AlphaZero. 

I'd love to chat over Zoom if you're interested to see if there's a good fit. I will also be returning to UVA on March 29-30th, so 
I'd also enjoy the opportunity to talk in person.

Thank you, 


##### Draft 3 - Shangton Zhang

Title: [e5274d52b4c6] Undergraduate Research Intern Inquiry

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm on a fellowship away from Grounds through the [Neo Scholars](https://neo.com/scholars) program
which has allowed me to spend the past few months exploring my interests in machine learning and reinforcement learning. I want
to return to Grounds in the fall of 2025 and contribute to your research as an intern.

Why you'd benefit from me: 
- I'm willing to dedicate 40+ hours/week to research since I will be a part-time student for the entire year.
- I'm willing to spend the rest of my fellowship and summer spinning up in RL, working alongside you.
- I have experience in industry working as a software engineer 
at [Principal Financial Group](https://www.principal.com/) and [Vercel](https://vercel.com), so 
I can contribute without hand-holding.

Why me? 
- I'm curious to explore research in RL
- I am already up to speed with basics of RL and the math that motivates it. 
- I've already worked with ML/RL in the past: 
    - Fine-tuned diffusion models (on Rivanna).
    - Served ML workloads with Ray. 
    - Completed simple RL experiments with OpenAI gymnasium. 

What I'm working on to get up to speed:
- OpenAI's Spinning Up in RL.
- Sutton and Barto's Textbook (accompanied with your GitHub repo of the Python replication).
- Building towards an implementation of AlphaZero. 

I'd love to chat over Zoom if you're interested to see if there's a good fit. I will also be returning to UVA on March 29-30th, so 
I'd also enjoy the opportunity to talk in person.

Thank you,


##### Draft 4 - Shangton Zhang

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm on a fellowship away from Grounds through the [Neo Scholars](https://neo.com/scholars) program
which has allowed me to spend the past few months exploring my interests in machine learning and reinforcement learning. I want
to return to Grounds in the fall of 2025 and contribute to your research as an intern.

Why you'd benefit from me?
- I will be a part-time student for an entire year and therefore will work incredibly hard on your research.
- I am willing to spend the rest of my fellowship and summer spinning up in RL, working alongside you.
- I have experience in industry working as a software engineer 
at [Principal Financial Group](https://www.principal.com/) and [Vercel](https://vercel.com), so 
I can contribute without hand-holding.

Why pick me?
- I'm interested in Amir Moeini's work with mathematical benchmarking. I've recently explored automated theorem proving (ATP) which 
has had a lot of movement recently, including AlphaProof. Most labs are focusing on LLM-based approaches (DeepSeek with DeepSeek-Math, Harmonic.ai), but 
few besides DeepMind are exploring RL-based approaches. In early 2025, labs have reached SOTA on MiniF2F using MCTS and/or Best-First Tree Search approaches, 
so I believe there's opportunity to move quickly and use interesting RL techniques for ATP. I'm incredibly interested in mathematical superintelligence, 
and it seems to align with Amir and your research direction. 
- I've worked with ML/RL in the past:
    - Fine-tuned diffusion models (on Rivanna).
    - Served ML workloads with Ray. 
    - Completed simple RL experiments with OpenAI gymnasium. 
- I'm a fast learner.

What I'm working on to get up to speed?
- OpenAI's Spinning Up in RL.
- Sutton and Barto's Textbook (accompanied with your GitHub repo of the Python replication).

I'd love to chat over Zoom if you're interested to see if there's a good fit. I will also be returning to UVA on March 29-30th, so 
I'd also enjoy the opportunity to talk in person.


### [Email to Aidong Zhang (aidong@virginia.edu)](https://www.cs.virginia.edu/~az9eg/website/home.html)

#### Draft 1

Hi Professor Zhang, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm on a fellowship through the 
[Neo Scholars](https://neo.com/scholars) program and have spent the past few months exploring my interests in machine learning, and
I'm reaching out to see if there is an opportunity for me to intern for you. I'm partiularly interested in your work in federated learning. 

I saw on your website that you're looking for PhD students, but I'm wondering if there's an opporuntity for an undegraduate to intern. 

Why I'd be useful for you:
- I'm going to be a part-time student for an entire year, so I'm willing to dedicate 40+ hours/week to research. 
- I have experience in industry working as a software engineer and am able to contribute without hand-holding. 
- I will be spending the rest of my fellowship and summer spinning up in machine learning, and am willing to work alongside you to 
get up to speed so we can work together in F2025. 

Why you should talk to me:
- I'm curious about your work in federated learning. 
- I have experience in ML/RL:
    - Served ML workloads with Ray. 
    - Finetuned diffusion models (on Rivanna).
- I'm a fast learner.

I'd love to chat over Zoom if you're interested to see if there's a good fit. I will also be returning to UVA on March 29-30th, so 
I'd also enjoy the opportunity to talk in person.

Thank you,

### Email to Chen-Yu Wei [(chenyu.wei@virginia.edu)](https://bahh723.github.io/)


#### Draft 1

Hi Professor Wei, 

I'm Charlie Meyer, an undergrad CS student at UVA. I'm on a fellowship through the 
[Neo Scholars](https://neo.com/scholars) program and have spent the past few months exploring my interests in machine learning and reinforcement learning, and
I'm reaching out to see if there is an opportunity for me to intern for you. Although I have little experience in ML/RL, I'm incredibly curious and eager 
to learn with your guidance. 

Why I'd be useful for you:
- I will be a part-time student for an entire year, so I'm able to work incredibly hard on supporting your research. 
- I have experience in industry working as a software engineer and am able to spin-up quickly without hand-holding. 
    - Worked at [Principal Financial Group](https://www.principal.com/) scaling AWS infrastructure as code, and [Scenthound](https://www.scenthound.com/) as a full-stack developer. 
    - Launched multiple SaaS products (https://www.hufflo.com, https://www.simpletext.dev) in 2-week sprints. 
    - Will work at [Vercel](https://vercel.com) in SF this summer. 
- I have expereince in ML:
    - Served ML workloads with Ray. 
    - Finetuned diffusion models (on Rivanna).
    - Completed simple RL experiments with OpenAI gymnasium. 

What I'm interested in in your lab:
- I was incredibly impressed by the work on the regret bound you created in adversarial linear MDPs, along with the work that built
upon DEC for a hybrid model type. I would love to talk with Haolin about their work!
- I saw that Kingsley is working on RLHF methods with Haolin. I've read recent surveys about LM persuasion and papers guaranteeing LM truthfulness within certian bounds, and am 
interested in the application of RLHF methods. 
- As mentioned on your website, I'm also interested in practical applications of RL. I'd love to talk about an emerging use case for RL: automated theorem proving. 
With improvements in synthetic data, labs starting to make progress in theorem proving (DeepMind with AlphaProof, DeepSeek with DeepSeek-Math and DeepSeek-Prove), and LLMs improved ability to generate Lean 4, I think there's a great opportunity to use RL for 
mathematical superintelligence (rather than LLM-based approaches).

What I'm working on to get up to speed:
- OpenAI's Spinning Up in RL.
- Sutton and Barto's Textbook (accompanied with your GitHub repo of the python replication).

I'd love to work with you throughout the end of my fellowship, throughout the summer, and then into the fall of 2025 if you are interested. 
Let me know if you'd like to chat over Zoom to see if there's a good fit. I will also be returning to UVA on March 29-30th, so 
I'd also enjoy the opportunity to talk in person.

Thank you,

###